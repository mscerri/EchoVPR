{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ..\\src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import isfile, join\n",
    "\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import TensorDataset, Subset\n",
    "import numpy as np\n",
    "\n",
    "from configs.utils import get_config, get_bool_from_config\n",
    "from echovpr.datasets.utils import get_dataset, get_subset_dataset\n",
    "from echovpr.models.utils import get_sparsity\n",
    "from echovpr.models.single_esn import SingleESN\n",
    "from echovpr.models.sparce_layer import SpaRCe\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config(\"configs\\\\train_esn_nordland_full.ini\", log=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact_name = 'uos_ml/echovpr/esn_9km0ic3z:v0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init()\n",
    "artifact = run.use_artifact(artifact_name, type='model')\n",
    "artifact_dir = artifact.download()\n",
    "\n",
    "model_file = join(artifact_dir, 'model.pt')\n",
    "esn_model_file = join(artifact_dir, 'esn_model.pt')\n",
    "\n",
    "all_in_one = not isfile(esn_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_features=int(config['model_in_features'])\n",
    "reservoir_size=int(config['model_reservoir_size'])\n",
    "out_features=int(config['model_out_features'])\n",
    "\n",
    "esn_alpha = float(config['model_esn_alpha'])\n",
    "esn_gamma = float(config['model_esn_gamma'])\n",
    "esn_rho = float(config['model_esn_rho'])\n",
    "esn_num_connections = int(config['model_esn_num_connections'])\n",
    "sparce_enabled = get_bool_from_config(config, 'model_sparce_enabled')\n",
    "\n",
    "model = nn.ModuleDict()\n",
    "\n",
    "esn_model = SingleESN(\n",
    "  in_features, \n",
    "  reservoir_size, \n",
    "  alpha=esn_alpha, \n",
    "  gamma=esn_gamma, \n",
    "  rho=esn_rho,\n",
    "  sparsity=get_sparsity(esn_num_connections, reservoir_size),\n",
    "  device=device\n",
    ")\n",
    "\n",
    "if all_in_one:\n",
    "  model[\"esn\"] = esn_model\n",
    "\n",
    "if sparce_enabled:\n",
    "  model[\"sparce\"] = SpaRCe(reservoir_size)\n",
    "\n",
    "model[\"out\"] = nn.Linear(in_features=reservoir_size, out_features=out_features, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not all_in_one:\n",
    "  esn_model.load_state_dict(torch.load(esn_model_file))\n",
    "\n",
    "model.load_state_dict(torch.load(model_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not all_in_one:\n",
    "  esn_model.eval().to(device)\n",
    "  \n",
    "model.eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summer_dataset = get_dataset(config['dataset_nordland_summer_hidden_repr_file_path'])\n",
    "winter_dataset = get_dataset(config['dataset_nordland_winter_hidden_repr_file_path'])\n",
    "\n",
    "max_n = summer_dataset.tensors[0].max()\n",
    "_ = summer_dataset.tensors[0].divide_(max_n)\n",
    "_ = winter_dataset.tensors[0].divide_(max_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(model, dataLoader, device: torch.device):\n",
    "    x_processed_list = []\n",
    "    y_target_list = []\n",
    "    \n",
    "    for x, y_target in dataLoader:\n",
    "        x = x.to(device)\n",
    "        x_processed = model(x)\n",
    "\n",
    "        x_processed_list.append(x_processed.cpu())\n",
    "        y_target_list.append(y_target)\n",
    "\n",
    "    return (torch.vstack(x_processed_list), torch.vstack(y_target_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_esn(model, dataLoader, sparce_enabled, quantiles, top_k = 100):\n",
    "    predictions = []\n",
    "    ground_truths = []\n",
    "\n",
    "    with torch.no_grad():    \n",
    "        for x, y_target in dataLoader:\n",
    "\n",
    "            x = x.to(device)\n",
    "            \n",
    "            if sparce_enabled:\n",
    "                x = model[\"sparce\"](x, quantiles)\n",
    "\n",
    "            preds = model[\"out\"](x)\n",
    "\n",
    "            _, predIdx = torch.topk(preds, top_k, dim=1)\n",
    "\n",
    "            predictions.append(predIdx.cpu())\n",
    "            ground_truths.append(torch.argmax(y_target, dim=1, keepdim=True))\n",
    "\n",
    "    return (torch.vstack(predictions), torch.vstack(ground_truths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = len(winter_dataset)\n",
    "dataset_tolerance = 10\n",
    "n_values = [1, 5, 10, 20, 50, 100]\n",
    "\n",
    "def get_positives(gt, dataset_tolerance, dataset_size):\n",
    "    return [list(filter(lambda x: (x >= 0 and x < dataset_size), range(i.item() - dataset_tolerance, i.item() + dataset_tolerance + 1))) for i in gt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p(starting_point, sim_length, esn_model, model, sparce_enabled, winter_dataset, gt, n_values, device):\n",
    "    with torch.no_grad():\n",
    "        # start from a different point\n",
    "        winter_dataset_subset = Subset(winter_dataset, range(starting_point, starting_point + sim_length))\n",
    "        gt = gt[starting_point:]\n",
    "        \n",
    "        print(f\"Winter dataset size: {len(winter_dataset_subset)}\")\n",
    "\n",
    "        winter_dataLoader = DataLoader(winter_dataset_subset, num_workers=int(config['dataloader_threads']), batch_size=int(config['train_batchsize']), shuffle=False)\n",
    "\n",
    "        winter_esn_dataset = TensorDataset(*process(esn_model, winter_dataLoader, device))\n",
    "        \n",
    "        winter_dataset_quantiles = None\n",
    "\n",
    "        if sparce_enabled:\n",
    "            # Calculate Training Dataset Quantiles\n",
    "            quantile = float(config['model_sparce_quantile'])\n",
    "            winter_dataset_quantiles = torch.quantile(torch.abs(torch.vstack([t[0] for t in winter_esn_dataset])), quantile, dim=0).to(device)\n",
    "\n",
    "        dataLoader = DataLoader(winter_esn_dataset, num_workers=int(config['dataloader_threads']), batch_size=int(config['train_batchsize']), shuffle=False)\n",
    "        predictions = TensorDataset(*eval_esn(model, dataLoader, sparce_enabled, winter_dataset_quantiles))\n",
    "\n",
    "        correct_at_n = np.zeros((len(predictions), len(n_values)))\n",
    "\n",
    "        for qIx, pred in enumerate(predictions.tensors[0]):\n",
    "            for i, n in enumerate(n_values):\n",
    "                # if in top N then also in top NN, where NN > N\n",
    "                if np.any(np.in1d(pred[:n], gt[qIx])):\n",
    "                    correct_at_n[qIx, i:] += 1\n",
    "                    break\n",
    "\n",
    "        del winter_dataset_subset\n",
    "        del winter_dataLoader\n",
    "        del winter_esn_dataset\n",
    "        del dataLoader\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return correct_at_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = get_positives(torch.argmax(winter_dataset.tensors[1], dim=1), dataset_tolerance, dataset_size)\n",
    "\n",
    "# correct_at_n = p(0, esn_model, model, sparce_enabled, winter_dataset, gt, n_values, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_positions = np.linspace(0, dataset_size - 10000, 500).astype(int)\n",
    "sim_length = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lists = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for starting_point in starting_positions[160:]:\n",
    "    print(f\"Starting point: {starting_point}\")\n",
    "    correct_at_n = p(starting_point, sim_length, esn_model, model, sparce_enabled, winter_dataset, gt, n_values, device)\n",
    "    lists.append(correct_at_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('results/multiple_startingpoints_340.npz', starting_positions=starting_positions[160:], sim_length=sim_length, n_values=n_values, correct_at_n=lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "results = np.load('results/multiple_startingpoints_full.npz')\n",
    "correct_at_n_list = results['correct_at_n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_correct_at_n = np.zeros((sim_length, len(n_values)))\n",
    "\n",
    "for i, correct_at_n in enumerate(correct_at_n_list):\n",
    "    final_correct_at_n += correct_at_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_c = final_correct_at_n / len(correct_at_n_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(f_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, _ in enumerate(n_values):\n",
    "    plt.plot(np.convolve(f_c[:, i], np.ones(20)/20, mode='valid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.convolve(f_c[:200, 5], np.ones(20)/20, mode='valid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_values = [1, 5, 10, 20, 50, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.convolve(lists[3][:1000, 0], np.ones(20)/20, mode='valid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(lists[0][-10000:, 0]))\n",
    "print(np.mean(lists[1][-10000:, 0]))\n",
    "print(np.mean(lists[2][-10000:, 0]))\n",
    "print(np.mean(lists[3][-10000:, 0]))\n",
    "print(np.mean(lists[4][:, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1c3ece5540a0c2ffc2d29a240d66f75bd95c704c907bef09022a1d843f0a91ef"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('patchnetvlad': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
