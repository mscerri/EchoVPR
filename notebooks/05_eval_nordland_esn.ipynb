{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mscer\\dev\\EchoVPR\\src\n",
      "running develop\n",
      "running egg_info\n",
      "writing echovpr.egg-info\\PKG-INFO\n",
      "writing dependency_links to echovpr.egg-info\\dependency_links.txt\n",
      "writing top-level names to echovpr.egg-info\\top_level.txt\n",
      "reading manifest file 'echovpr.egg-info\\SOURCES.txt'\n",
      "writing manifest file 'echovpr.egg-info\\SOURCES.txt'\n",
      "running build_ext\n",
      "Creating c:\\users\\mscer\\anaconda3\\envs\\patchnetvlad\\lib\\site-packages\\echovpr.egg-link (link to .)\n",
      "echovpr 1.0 is already the active version in easy-install.pth\n",
      "\n",
      "Installed c:\\users\\mscer\\dev\\echovpr\\src\n",
      "Processing dependencies for echovpr==1.0\n",
      "Finished processing dependencies for echovpr==1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mscer\\anaconda3\\envs\\patchnetvlad\\lib\\site-packages\\setuptools\\command\\easy_install.py:156: EasyInstallDeprecationWarning: easy_install command is deprecated. Use build and pip and other standards-based tools.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mscer\\anaconda3\\envs\\patchnetvlad\\lib\\site-packages\\setuptools\\command\\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%cd ..\\src\n",
    "!python setup.py develop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import isfile, join\n",
    "\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "from configs.utils import get_config, get_bool_from_config\n",
    "from echovpr.datasets.utils import get_dataset, get_subset_dataset\n",
    "from echovpr.models.utils import get_sparsity\n",
    "from echovpr.models.single_esn import SingleESN\n",
    "from echovpr.models.sparce_layer import SpaRCe\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config(\"configs\\\\train_esn_nordland_full_sweep.ini\", log=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact_name = 'uos_ml/echovpr/esn_4wkrv7z1:v0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmscerri\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/mscerri/EchoVPR/runs/1sr49dqg\" target=\"_blank\">jolly-smoke-16</a></strong> to <a href=\"https://wandb.ai/mscerri/EchoVPR\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact esn_4wkrv7z1:v0, 1101.55MB. 1 files... Done. 0:0:0\n"
     ]
    }
   ],
   "source": [
    "run = wandb.init()\n",
    "artifact = run.use_artifact(artifact_name, type='model')\n",
    "artifact_dir = artifact.download()\n",
    "\n",
    "model_file = join(artifact_dir, 'model.pt')\n",
    "esn_model_file = join(artifact_dir, 'esn_model.pt')\n",
    "\n",
    "all_in_one = not isfile(esn_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_features=int(config['model_in_features'])\n",
    "reservoir_size=int(config['model_reservoir_size'])\n",
    "out_features=int(config['model_out_features'])\n",
    "\n",
    "esn_alpha = float(config['model_esn_alpha'])\n",
    "esn_gamma = float(config['model_esn_gamma'])\n",
    "esn_rho = float(config['model_esn_rho'])\n",
    "esn_num_connections = int(config['model_esn_num_connections'])\n",
    "sparce_enabled = get_bool_from_config(config, 'model_sparce_enabled')\n",
    "\n",
    "model = nn.ModuleDict()\n",
    "\n",
    "esn_model = SingleESN(\n",
    "  in_features, \n",
    "  reservoir_size, \n",
    "  alpha=esn_alpha, \n",
    "  gamma=esn_gamma, \n",
    "  rho=esn_rho,\n",
    "  sparsity=get_sparsity(esn_num_connections, reservoir_size),\n",
    "  device=device\n",
    ")\n",
    "\n",
    "if all_in_one:\n",
    "  model[\"esn\"] = esn_model\n",
    "\n",
    "if sparce_enabled:\n",
    "  model[\"sparce\"] = SpaRCe(reservoir_size)\n",
    "\n",
    "model[\"out\"] = nn.Linear(in_features=reservoir_size, out_features=out_features, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not all_in_one:\n",
    "  esn_model.load_state_dict(torch.load(esn_model_file))\n",
    "\n",
    "model.load_state_dict(torch.load(model_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleDict(\n",
       "  (esn): SingleESN(in_features=500, out_features=8000)\n",
       "  (out): Linear(in_features=8000, out_features=27592, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not all_in_one:\n",
    "  esn_model.eval().to(device)\n",
    "  \n",
    "model.eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "summer_dataset = get_dataset(config['dataset_nordland_summer_hidden_repr_file_path'])\n",
    "winter_dataset = get_dataset(config['dataset_nordland_winter_hidden_repr_file_path'])\n",
    "\n",
    "max_n = summer_dataset.tensors[0].max()\n",
    "_ = summer_dataset.tensors[0].divide_(max_n)\n",
    "_ = winter_dataset.tensors[0].divide_(max_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(model, dataLoader, device: torch.device):\n",
    "    x_processed_list = []\n",
    "    y_target_list = []\n",
    "    \n",
    "    for x, y_target in dataLoader:\n",
    "        x = x.to(device)\n",
    "        x_processed = model(x)\n",
    "\n",
    "        x_processed_list.append(x_processed.cpu())\n",
    "        y_target_list.append(y_target)\n",
    "\n",
    "    return (torch.vstack(x_processed_list), torch.vstack(y_target_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winter dataset size: 27592\n"
     ]
    }
   ],
   "source": [
    "print(f\"Winter dataset size: {len(winter_dataset)}\")\n",
    "winter_dataLoader = DataLoader(winter_dataset, num_workers=int(config['dataloader_threads']), batch_size=int(config['train_batchsize']), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "winter_dataset = TensorDataset(*process(esn_model, winter_dataLoader, device))\n",
    "dataset_size = len(winter_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation dataset size: 2759\n",
      "Test dataset size: 24833\n"
     ]
    }
   ],
   "source": [
    "# Prepare Datasets\n",
    "\n",
    "val_dataset = get_subset_dataset(winter_dataset, config['dataset_nordland_winter_val_limit_indices_file_path'])\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "val_dataLoader = DataLoader(val_dataset, num_workers=int(config['dataloader_threads']), batch_size=int(config['train_batchsize']), shuffle=False)\n",
    "\n",
    "test_dataset = get_subset_dataset(winter_dataset, config['dataset_nordland_winter_test_limit_indices_file_path'])\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "test_dataLoader = DataLoader(test_dataset, num_workers=int(config['dataloader_threads']), batch_size=int(config['train_batchsize']), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset_quantiles = None\n",
    "\n",
    "if sparce_enabled:\n",
    "    # Calculate Training Dataset Quantiles\n",
    "    quantile = float(config['model_sparce_quantile'])\n",
    "    val_dataset_quantiles = torch.quantile(torch.abs(torch.vstack([t[0] for t in val_dataset])), quantile, dim=0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, dataLoader, sparce_enabled, quantiles):\n",
    "    predictions = []\n",
    "    ground_truths = []\n",
    "\n",
    "    with torch.no_grad():    \n",
    "        for x, y_target in dataLoader:\n",
    "\n",
    "            x = x.to(device)\n",
    "            \n",
    "            if sparce_enabled:\n",
    "                x = model[\"sparce\"](x, quantiles)\n",
    "\n",
    "            preds = model[\"out\"](x)\n",
    "\n",
    "            predictions.append(preds.cpu())\n",
    "            ground_truths.append(y_target)\n",
    "\n",
    "    return (torch.vstack(predictions), torch.vstack(ground_truths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds = TensorDataset(*get_predictions(model, val_dataLoader, sparce_enabled, val_dataset_quantiles))\n",
    "test_preds = TensorDataset(*get_predictions(model, test_dataLoader, sparce_enabled, val_dataset_quantiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_top_k(predictions, top_k = 100):\n",
    "    top_k_predictions = []\n",
    "    ground_truths = []\n",
    "\n",
    "    for  _, (preds, y_target) in enumerate(predictions):\n",
    "\n",
    "        _, predIdx = torch.topk(preds, top_k)\n",
    "\n",
    "        top_k_predictions.append(predIdx.cpu())\n",
    "        ground_truths.append(torch.argmax(y_target, keepdim=True))\n",
    "\n",
    "    return (torch.vstack(top_k_predictions), torch.vstack(ground_truths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_top_k_preds = TensorDataset(*eval_top_k(val_preds))\n",
    "test_top_k_preds = TensorDataset(*eval_top_k(test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory cleanup\n",
    "del model\n",
    "del val_dataset\n",
    "del val_dataLoader\n",
    "del test_dataset\n",
    "del test_dataLoader\n",
    "del winter_dataset\n",
    "del winter_dataLoader\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_recall(gt, predictions, numQ, n_values, recall_str=''):\n",
    "    correct_at_n = np.zeros(len(n_values))\n",
    "    for qIx, pred in enumerate(predictions):\n",
    "        for i, n in enumerate(n_values):\n",
    "            # if in top N then also in top NN, where NN > N\n",
    "            if np.any(np.in1d(pred[:n], gt[qIx])):\n",
    "                correct_at_n[i:] += 1\n",
    "                break\n",
    "    recall_at_n = correct_at_n / numQ\n",
    "    all_recalls = {}  # make dict for output\n",
    "    for i, n in enumerate(n_values):\n",
    "        all_recalls[n] = recall_at_n[i]\n",
    "        print(\"====> Recall {}@{}: {:.4f}\".format(recall_str, n, recall_at_n[i]))\n",
    "    return all_recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = 27592\n",
    "dataset_tolerance = 10\n",
    "\n",
    "def get_positives(gt, dataset_tolerance, dataset_size):\n",
    "    return [list(filter(lambda x: (x >= 0 and x < dataset_size), range(i.item() - dataset_tolerance, i.item() + dataset_tolerance + 1))) for i in gt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Recall for Val EchoVPR@1: 0.3994\n",
      "====> Recall for Val EchoVPR@5: 0.5531\n",
      "====> Recall for Val EchoVPR@10: 0.6086\n",
      "====> Recall for Val EchoVPR@20: 0.6658\n",
      "====> Recall for Val EchoVPR@50: 0.7441\n",
      "====> Recall for Val EchoVPR@100: 0.7978\n",
      "====> Recall for Test EchoVPR@1: 0.3990\n",
      "====> Recall for Test EchoVPR@5: 0.5480\n",
      "====> Recall for Test EchoVPR@10: 0.6089\n",
      "====> Recall for Test EchoVPR@20: 0.6686\n",
      "====> Recall for Test EchoVPR@50: 0.7457\n",
      "====> Recall for Test EchoVPR@100: 0.7989\n"
     ]
    }
   ],
   "source": [
    "n_values = [1, 5, 10, 20, 50, 100]\n",
    "gt_val = get_positives(val_top_k_preds.tensors[1], dataset_tolerance, dataset_size)\n",
    "val_recalls = compute_recall(gt_val, val_top_k_preds.tensors[0], len(val_top_k_preds), n_values, 'for Val EchoVPR')\n",
    "gt_test = get_positives(test_top_k_preds.tensors[1], dataset_tolerance, dataset_size)\n",
    "test_recalls = compute_recall(gt_test, test_top_k_preds.tensors[0], len(test_top_k_preds), n_values, 'for Test EchoVPR')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "57baa5815c940fdaff4d14510622de9616cae602444507ba5d0b6727c008cbd6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
