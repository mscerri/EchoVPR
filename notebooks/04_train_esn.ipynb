{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ..\\src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import wandb\n",
    "from configs.utils import (get_bool_from_config, get_config_wandb, update_config_wandb, get_config, update_config,\n",
    "                           get_float_from_config, get_int_from_config)\n",
    "from echovpr.models.single_esn import SingleESN\n",
    "from echovpr.models.sparce_layer import SpaRCe\n",
    "from echovpr.models.utils import get_sparsity\n",
    "from echovpr.trainer.eval import run_eval\n",
    "from echovpr.trainer.metrics.recall import compute_recall\n",
    "from echovpr.trainer.prepare_esn_datasets import prepare_esn_datasets\n",
    "from echovpr.trainer.prepare_final_datasets import prepare_final_datasets\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "log = logging.getLogger()\n",
    "\n",
    "env_torch_device = os.environ.get(\"TORCH_DEVICE\")\n",
    "if env_torch_device is not None:\n",
    "    device = torch.device(env_torch_device)\n",
    "    log.info(f'Setting device set by environment to {env_torch_device}')\n",
    "else:\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")   \n",
    "    log.info('Setting default available device')\n",
    "\n",
    "# os.environ[\"WANDB_SILENT\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run, config = get_config_wandb('configs\\\\train_esn_nordland_full.ini', project='echovpr_nordland', logger=log, log=True)\n",
    "\n",
    "# run, config = get_config_wandb('configs\\\\train_esn_nordland_continuity.ini', project='echovpr_nordland', logger=log, log=True)\n",
    "\n",
    "# run, config = get_config_wandb('configs\\\\train_esn_oxford.ini', project='echovpr_nordland', logger=log, log=True)\n",
    "\n",
    "# config = update_config(run, {\n",
    "#     'model_sparce_enabled': 'True',\n",
    "#     'model_sparce_quantile': 0.7,\n",
    "#     'model_esn_alpha': 0.7,\n",
    "#     'model_esn_gamma': 0.99,\n",
    "#     'model_esn_num_connections': 20,\n",
    "#     'train_lr': 0.0005,\n",
    "#     'continuity': 10\n",
    "# }, log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(get_config('configs\\\\train_esn_nordland_full.ini', logger=log).items('main'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = update_config(config, {\n",
    "    'model_sparce_enabled': 'False',\n",
    "    'model_sparce_quantile': 0.7,\n",
    "    'model_esn_alpha': 1.0,\n",
    "    'model_esn_gamma': 0.01,\n",
    "    'model_esn_rho': 0.99,\n",
    "    'model_esn_num_connections': 20,\n",
    "    'train_lr': 0.0005,\n",
    "    'train_lr_sparce_divide_by': 1000,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_forward(model, x, kwargs):\n",
    "    if kwargs['sparce_enabled']:\n",
    "        x = model[\"sparce\"](x, kwargs['dataset_quantiles'])\n",
    "    \n",
    "    y = model[\"out\"](x)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup ESN\n",
    "in_features=int(config['model_in_features'])\n",
    "reservoir_size=int(config['model_reservoir_size'])\n",
    "out_features=int(config['model_out_features'])\n",
    "\n",
    "esn_alpha = float(config['model_esn_alpha'])\n",
    "esn_gamma = float(config['model_esn_gamma'])\n",
    "esn_rho = float(config['model_esn_rho'])\n",
    "esn_num_connections = int(config['model_esn_num_connections'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init models\n",
    "# esn_path = os.path.join(wandb.run.dir, 'esn_model.pt')\n",
    "\n",
    "model_esn = SingleESN(\n",
    "    in_features, \n",
    "    reservoir_size, \n",
    "    alpha=esn_alpha, \n",
    "    gamma=esn_gamma, \n",
    "    rho=esn_rho,\n",
    "    sparsity=get_sparsity(esn_num_connections, reservoir_size),\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Save ESN Model\n",
    "\n",
    "# model_artifact = wandb.Artifact(f'esn_{run.id}', \"model\", metadata=config)\n",
    "# torch.save(model_esn.state_dict(), esn_path)\n",
    "# model_artifact.add_file(esn_path)\n",
    "\n",
    "# Move to device\n",
    "\n",
    "model_esn.to(device)\n",
    "\n",
    "# Load datasets, normalize and process through ESN\n",
    "\n",
    "esn_descriptors = prepare_esn_datasets(model_esn, config, device, log)\n",
    "\n",
    "del model_esn\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = update_config(config, {\n",
    "    'model_sparce_enabled': 'True',\n",
    "    'model_sparce_quantile': 0.3,\n",
    "    'model_esn_alpha': 1.0,\n",
    "    'model_esn_gamma': 0.01,\n",
    "    'model_esn_rho': 0.99,\n",
    "    'model_esn_num_connections': 20,\n",
    "    'train_lr': 0.0005,\n",
    "    'train_lr_sparce_divide_by': 1000,\n",
    "    'train_batchsize': 200,\n",
    "    'train_max_epochs': 100,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Final Datasets\n",
    "train_dataset, train_dataLoader, val_dataset, val_dataLoader, test_dataLoader, train_gt, eval_gt = prepare_final_datasets(esn_descriptors, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparce_enabled = get_bool_from_config(config, 'model_sparce_enabled')\n",
    "print(sparce_enabled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_quantiles = None\n",
    "val_dataset_quantiles = None\n",
    "if sparce_enabled:\n",
    "    # Calculate Training Dataset Quantiles\n",
    "    quantile = float(config['model_sparce_quantile'])\n",
    "    train_dataset_quantiles = torch.quantile(torch.abs(train_dataset.tensors[0]), quantile, dim=0).to(device)\n",
    "    val_dataset_quantiles = torch.quantile(torch.abs(torch.vstack([t[0] for t in val_dataset])), quantile, dim=0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.ModuleDict()\n",
    "\n",
    "if sparce_enabled:\n",
    "    model[\"sparce\"] = SpaRCe(reservoir_size)\n",
    "\n",
    "model[\"out\"] = nn.Linear(in_features=reservoir_size, out_features=out_features, bias=True)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "optimizer_params = []\n",
    "\n",
    "lr = float(config['train_lr'])    \n",
    "\n",
    "if sparce_enabled:\n",
    "    lr_sparce = lr / get_int_from_config(config, 'train_lr_sparce_divide_by', 1000)\n",
    "    optimizer_params.append({'params': model[\"sparce\"].parameters(), 'lr': lr_sparce})\n",
    "\n",
    "optimizer_params.append({'params': model[\"out\"].parameters()})\n",
    "\n",
    "optimizer = torch.optim.Adam(optimizer_params, lr=lr)\n",
    "criterion = nn.BCEWithLogitsLoss(reduction='mean').to(device)\n",
    "\n",
    "# Watch Model\n",
    "# wandb.watch(model, criterion=criterion, log=\"all\", idx=1, log_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup metrics\n",
    "n_values = [1, 5, 10, 20, 50, 100]\n",
    "top_k = max(n_values)\n",
    "\n",
    "# Training loop\n",
    "early_stopping_enabled = get_bool_from_config(config, 'early_stopping_enabled', False)\n",
    "early_stopping_patience = get_int_from_config(config, 'early_stopping_patience', 10)\n",
    "early_stopping_min_delta = get_float_from_config(config, 'early_stopping_min_delta', 0.01)\n",
    "\n",
    "# best_model_path = os.path.join(wandb.run.dir, 'model.pt')\n",
    "\n",
    "max_epochs = get_int_from_config(config, 'train_max_epochs', 1)\n",
    "num_batches = len(train_dataLoader)\n",
    "\n",
    "steps = 0\n",
    "\n",
    "previous_val_recall_at_1 = 0\n",
    "best_val_recall_dic = {\n",
    "    1: 0,\n",
    "}\n",
    "best_test_recall_dic = {}\n",
    "not_improved_epochs = 0    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(0, max_epochs + 1):        \n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    for x, y_target, y_idx in train_dataLoader:\n",
    "        steps += 1\n",
    "\n",
    "        x = x.to(device)\n",
    "        y_target = y_target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if sparce_enabled:\n",
    "            x = model[\"sparce\"](x, train_dataset_quantiles)\n",
    "        \n",
    "        y = model[\"out\"](x)\n",
    "\n",
    "        loss = criterion(y, y_target)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            _, predIdx = torch.topk(y, top_k)\n",
    "            predictions += zip(y_idx.numpy(), predIdx.cpu().numpy())\n",
    "\n",
    "        batch_loss = loss.item()\n",
    "    \n",
    "        epoch_loss += batch_loss\n",
    "    \n",
    "    train_recalls = compute_recall(train_gt, predictions, len(predictions), n_values)\n",
    "    \n",
    "    avg_loss = epoch_loss / num_batches\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_recalls, _ = run_eval(model, val_dataLoader, eval_gt, n_values, top_k, device, model_forward=model_forward, sparce_enabled=sparce_enabled, dataset_quantiles=val_dataset_quantiles)\n",
    "\n",
    "        current_val_recall_at_1 = val_recalls[1]\n",
    "\n",
    "        is_better = current_val_recall_at_1 > best_val_recall_dic[1]\n",
    "\n",
    "        if is_better:\n",
    "            best_val_recall_dic = val_recalls\n",
    "            # run.summary[\"best_val_recall@1\"] = best_val_recall_dic[1]\n",
    "            log.info(f\"Better val_recall@1 reached: {best_val_recall_dic[1]}\")\n",
    "            # torch.save(model.state_dict(), best_model_path)\n",
    "\n",
    "            # best_test_recall_dic, _ = run_eval(model, test_dataLoader, eval_gt, n_values, top_k, device, model_forward=model_forward, sparce_enabled=sparce_enabled, dataset_quantiles=val_dataset_quantiles)\n",
    "\n",
    "            # log.info(f\"Epoch {epoch}/{max_epochs} - Loss: {avg_loss:.8f} - Train Recall@1: {train_recalls[1]:.4f} - Val Recall@1: {best_val_recall_dic[1]:.4f} - Test Recall@1: {best_test_recall_dic[1]:.4f}\")\n",
    "            log.info(f\"Epoch {epoch}/{max_epochs} - Loss: {avg_loss:.8f} - Train Recall@1: {train_recalls[1]:.4f} - Val Recall@1: {best_val_recall_dic[1]:.4f}\")\n",
    "        else:\n",
    "            log.info(f\"Epoch {epoch}/{max_epochs} - Loss: {avg_loss:.8f} - Train Recall@1: {train_recalls[1]:.4f} - Val Recall@1: {val_recalls[1]:.4f}\")\n",
    "\n",
    "        if early_stopping_enabled:\n",
    "            if (current_val_recall_at_1 - previous_val_recall_at_1) > early_stopping_min_delta:\n",
    "                not_improved_epochs = 0\n",
    "            else:\n",
    "                not_improved_epochs += 1\n",
    "\n",
    "        previous_val_recall_at_1 = current_val_recall_at_1\n",
    "\n",
    "    # log_dic = {'train_loss': avg_loss, \"epoch\": epoch}\n",
    "\n",
    "    # for k, v in train_recalls.items():\n",
    "    #     log_dic[f\"train_recall@{k}\"] = v\n",
    "\n",
    "    # for k, v in val_recalls.items():\n",
    "    #     log_dic[f\"val_recall@{k}\"] = v\n",
    "\n",
    "    # for k, v in best_test_recall_dic.items():\n",
    "    #     log_dic[f\"test_recall@{k}\"] = v\n",
    "\n",
    "    # wandb.log(log_dic, step=steps)\n",
    "\n",
    "    if early_stopping_enabled and early_stopping_patience > 0 and not_improved_epochs > (early_stopping_patience / 1):\n",
    "        log.info(f'Performance did not improve for {early_stopping_patience} epochs. Stopping.')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save artifacts\n",
    "# model_artifact.add_file(best_model_path)\n",
    "# wandb.log_artifact(model_artifact, aliases=[\"best\"]) \n",
    "\n",
    "# # Finalise Summary\n",
    "# for key in best_val_recall_dic:\n",
    "#     best_key = f\"best_val_recall@{key}\"\n",
    "#     log.info(f\"Setting summary {best_key} reached: {best_val_recall_dic[key]}\")\n",
    "#     run.summary[best_key] = best_val_recall_dic[key]\n",
    "\n",
    "# for key in best_test_recall_dic:\n",
    "#     best_key = f\"best_test_recall@{key}\"\n",
    "#     log.info(f\"Setting summary {best_key} reached: {best_test_recall_dic[key]}\")\n",
    "#     run.summary[best_key] = best_test_recall_dic[key]\n",
    "\n",
    "# run.finish()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "57baa5815c940fdaff4d14510622de9616cae602444507ba5d0b6727c008cbd6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
