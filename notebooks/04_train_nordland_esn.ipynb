{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ..\\src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import TensorDataset\n",
    "from torchmetrics import MetricCollection\n",
    "\n",
    "from configs.utils import get_config, get_int_from_config, get_float_from_config, get_bool_from_config\n",
    "from echovpr.datasets.utils import get_dataset, get_subset_dataset, save_tensor\n",
    "from echovpr.models.utils import get_sparsity\n",
    "from echovpr.models.single_esn import SingleESN\n",
    "from echovpr.models.hier_esn import HierESN\n",
    "from echovpr.models.sparce_layer import SpaRCe\n",
    "from echovpr.trainer.metrics.recall_top_k_metric import RecallTopKMetric\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config(\"configs\\\\train_esn_nordland_full.ini\")\n",
    "# config = get_config(\"configs\\\\train_esn_nordland_1k.ini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init ESN and Lightning Modules\n",
    "\n",
    "in_features=int(config['model_in_features'])\n",
    "out_features=int(config['model_out_features'])\n",
    "esn_num_connections = int(config['model_esn_num_connections'])\n",
    "esn_hier = get_bool_from_config(config, 'model_esn_hier', False)\n",
    "sparce_enabled = get_bool_from_config(config, 'model_sparce_enabled')\n",
    "\n",
    "if not esn_hier:\n",
    "  reservoir_size=int(config['model_reservoir_size'])\n",
    "  esn_alpha = float(config['model_esn_alpha'])\n",
    "  esn_gamma = float(config['model_esn_gamma'])\n",
    "  esn_rho = float(config['model_esn_rho'])\n",
    "\n",
    "  model_esn = SingleESN(\n",
    "    in_features, \n",
    "    reservoir_size, \n",
    "    alpha=esn_alpha, \n",
    "    gamma=esn_gamma, \n",
    "    rho=esn_rho,\n",
    "    sparsity=get_sparsity(esn_num_connections, reservoir_size),\n",
    "    device=device\n",
    "  )\n",
    "else:\n",
    "  reservoir1_size=int(config['model_reservoir1_size'])\n",
    "  reservoir2_size=int(config['model_reservoir2_size'])\n",
    "\n",
    "  esn1_alpha = float(config['model_esn1_alpha'])\n",
    "  esn1_gamma = float(config['model_esn1_gamma'])\n",
    "  esn1_rho = float(config['model_esn1_rho'])\n",
    "\n",
    "  esn2_alpha = float(config['model_esn2_alpha'])\n",
    "  esn2_gamma = float(config['model_esn2_gamma'])\n",
    "  esn2_rho = float(config['model_esn2_rho'])\n",
    "\n",
    "  model_esn = HierESN(\n",
    "    in_features,\n",
    "    nReservoir1=reservoir1_size,\n",
    "    nReservoir2=reservoir2_size,\n",
    "    alpha1=esn1_alpha,\n",
    "    alpha2=esn2_alpha,\n",
    "    gamma1=esn1_gamma,\n",
    "    gamma2=esn2_gamma,\n",
    "    rho1=esn1_rho,\n",
    "    rho2=esn2_rho,\n",
    "    sparsity1=get_sparsity(esn_num_connections, reservoir1_size),\n",
    "    sparsity2=get_sparsity(esn_num_connections, reservoir2_size),\n",
    "    device=device\n",
    "  )\n",
    "  \n",
    "reservoir_output_size = reservoir1_size + reservoir2_size if esn_hier else reservoir_size\n",
    "\n",
    "model_esn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summer_dataset = get_dataset(config['dataset_nordland_summer_hidden_repr_file_path'])\n",
    "winter_dataset = get_dataset(config['dataset_nordland_winter_hidden_repr_file_path'])\n",
    "\n",
    "max_n = summer_dataset.tensors[0].max()\n",
    "_ = summer_dataset.tensors[0].divide_(max_n)\n",
    "_ = winter_dataset.tensors[0].divide_(max_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(model, dataLoader, device: torch.device):\n",
    "    x_processed_list = []\n",
    "    y_target_list = []\n",
    "    \n",
    "    for x, y_target in dataLoader:\n",
    "        x = x.to(device)\n",
    "        x_processed = model(x)\n",
    "\n",
    "        x_processed_list.append(x_processed.cpu())\n",
    "        y_target_list.append(y_target)\n",
    "\n",
    "    return (torch.vstack(x_processed_list), torch.vstack(y_target_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Summer dataset size: {len(summer_dataset)}\")\n",
    "summer_dataLoader = DataLoader(summer_dataset, num_workers=int(config['dataloader_threads']), batch_size=int(config['train_batchsize']), shuffle=False)\n",
    "\n",
    "print(f\"Winter dataset size: {len(winter_dataset)}\")\n",
    "winter_dataLoader = DataLoader(winter_dataset, num_workers=int(config['dataloader_threads']), batch_size=int(config['train_batchsize']), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nordland_summer_repr_x, nordland_summer_repr_y_target  = process(model_esn, summer_dataLoader, device)\n",
    "nordland_winter_repr_x, nordland_winter_repr_y_target = process(model_esn, winter_dataLoader, device)\n",
    "\n",
    "nordland_summer_repr_x_cpu = nordland_summer_repr_x.cpu()\n",
    "nordland_summer_repr_y_target_cpu = nordland_summer_repr_y_target.cpu()\n",
    "nordland_winter_repr_x_cpu = nordland_winter_repr_x.cpu()\n",
    "nordland_winter_repr_y_target_cpu = nordland_winter_repr_y_target.cpu()\n",
    "\n",
    "del nordland_summer_repr_x\n",
    "del nordland_summer_repr_y_target\n",
    "del nordland_winter_repr_x\n",
    "del nordland_winter_repr_y_target\n",
    "del summer_dataset\n",
    "del summer_dataLoader\n",
    "del winter_dataset\n",
    "del winter_dataLoader\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "summer_dataset = TensorDataset(nordland_summer_repr_x_cpu, nordland_summer_repr_y_target_cpu)\n",
    "winter_dataset = TensorDataset(nordland_winter_repr_x_cpu, nordland_winter_repr_y_target_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Datasets\n",
    "\n",
    "train_dataset = summer_dataset\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "train_dataLoader = DataLoader(train_dataset, num_workers=int(config['dataloader_threads']), batch_size=int(config['train_batchsize']), shuffle=True)\n",
    "\n",
    "val_dataset = get_subset_dataset(winter_dataset, config['dataset_nordland_winter_val_limit_indices_file_path'])\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "val_dataLoader = DataLoader(val_dataset, num_workers=int(config['dataloader_threads']), batch_size=int(config['train_batchsize']), shuffle=False)\n",
    "\n",
    "test_dataset = get_subset_dataset(winter_dataset, config['dataset_nordland_winter_test_limit_indices_file_path'])\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "test_dataLoader = DataLoader(test_dataset, num_workers=int(config['dataloader_threads']), batch_size=int(config['train_batchsize']), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.ModuleDict()\n",
    "\n",
    "if sparce_enabled:\n",
    "  model[\"sparce\"] = SpaRCe(reservoir_output_size)\n",
    "\n",
    "model[\"out\"] = nn.Linear(in_features=reservoir_output_size, out_features=out_features, bias=True)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_quantiles = None\n",
    "val_dataset_quantiles = None\n",
    "\n",
    "if sparce_enabled:\n",
    "    # Calculate Training Dataset Quantiles\n",
    "    quantile = float(config['model_sparce_quantile'])\n",
    "    train_dataset_quantiles = torch.quantile(torch.abs(train_dataset.tensors[0]), quantile, dim=0).to(device)\n",
    "    val_dataset_quantiles = torch.quantile(torch.abs(torch.vstack([t[0] for t in val_dataset])), quantile, dim=0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_params = []\n",
    "\n",
    "lr = float(config['train_lr'])\n",
    "\n",
    "if sparce_enabled:\n",
    "    lr_sparce = lr / get_int_from_config(config, 'train_lr_sparce_divide_by', 1000)\n",
    "    optimizer_params.append({'params': model[\"sparce\"].parameters(), 'lr': lr_sparce})\n",
    "\n",
    "optimizer_params.append({'params': model[\"out\"].parameters()})\n",
    "\n",
    "if config['train_optimizer'] == 'SGD':\n",
    "  optimizer = torch.optim.SGD(optimizer_params, lr=lr, momentum=float(config['train_momentum']), weight_decay=float(config['train_weight_decay']))\n",
    "  scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=int(config['train_lr_step']), gamma=float(config['train_lr_gamma']))\n",
    "else:\n",
    "  optimizer = torch.optim.Adam(optimizer_params, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss(reduction='mean').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, dataLoader, metrics, sparce_enabled, quantiles):\n",
    "    tolerance = 10\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for x, y_target in dataLoader:\n",
    "\n",
    "        x = x.to(device)\n",
    "        y_target = y_target.to(device)\n",
    "        \n",
    "        if sparce_enabled:\n",
    "            x = model[\"sparce\"](x, quantiles)\n",
    "\n",
    "        preds = model[\"out\"](x)\n",
    "\n",
    "        _, indices = torch.topk(preds, 100, dim=1)\n",
    "\n",
    "        distances = torch.abs(indices - torch.argmax(y_target, dim=1, keepdim=True))\n",
    "\n",
    "        correct += torch.sum(torch.sum(distances <= tolerance, dim=1) > 0)\n",
    "        total += x.shape[0]\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_tolerance = get_int_from_config(config, 'dataset_tolerance', 10)\n",
    "\n",
    "train_metrics = MetricCollection(\n",
    "{\n",
    "    'recall@1': RecallTopKMetric(top_k=1, tolerance=ds_tolerance),\n",
    "    'recall@5': RecallTopKMetric(top_k=5, tolerance=ds_tolerance),\n",
    "    'recall@10': RecallTopKMetric(top_k=10, tolerance=ds_tolerance),\n",
    "    'recall@20': RecallTopKMetric(top_k=20, tolerance=ds_tolerance),\n",
    "    'recall@50': RecallTopKMetric(top_k=50, tolerance=ds_tolerance),\n",
    "    'recall@100': RecallTopKMetric(top_k=100, tolerance=ds_tolerance)\n",
    "}, prefix='train_').to(device)\n",
    "val_metrics = train_metrics.clone(prefix='val_').to(device)\n",
    "test_metrics = train_metrics.clone(prefix='test_').to(device)\n",
    "\n",
    "best_val_recall_at_1 = 0\n",
    "save_best_checkpoint = True\n",
    "\n",
    "run_id = '8000_0.0005_1'\n",
    "\n",
    "for epoch in range(40):\n",
    "\n",
    "    train_metrics.reset()\n",
    "\n",
    "    for x, y_target in train_dataLoader:\n",
    "\n",
    "        x = x.to(device)\n",
    "        y_target = y_target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if sparce_enabled:\n",
    "            x = model[\"sparce\"](x, train_dataset_quantiles)\n",
    "        \n",
    "        y = model[\"out\"](x)\n",
    "\n",
    "        loss = criterion(y, y_target)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_metrics.update(y, y_target.int())\n",
    "    \n",
    "    if config['train_optimizer'] == 'SGD':\n",
    "        scheduler.step()\n",
    "\n",
    "    print(f\"Epoch: {epoch}, Loss: {loss.item()}, Train Metrics: {train_metrics.compute()}\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val_metrics_dic = eval(model, val_dataLoader, val_metrics, sparce_enabled, val_dataset_quantiles)\n",
    "        print(f\"Epoch: {epoch}, Val Metric: {val_metrics_dic}\")\n",
    "\n",
    "        current_val_recall_at_1 = val_metrics_dic['val_recall@1']\n",
    "\n",
    "        is_better = current_val_recall_at_1 > best_val_recall_at_1\n",
    "\n",
    "        if is_better:\n",
    "            test_metrics_dic = eval(model, test_dataLoader, test_metrics, sparce_enabled, val_dataset_quantiles)\n",
    "            print(f\"Epoch: {epoch}, Test Metric: {test_metrics_dic}\")\n",
    "            \n",
    "            if save_best_checkpoint:\n",
    "                save_tensor(model.state_dict(), f'checkpoints\\\\checkpoint_{run_id}.pt')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "57baa5815c940fdaff4d14510622de9616cae602444507ba5d0b6727c008cbd6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
